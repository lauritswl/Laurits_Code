**Bayesian data analysis** takes a *question* in the *form* of a *model* and uses **logic** to **produce** an *answer* in the *form* of *probability distributions*.

In modest terms, **Bayesian data analysis** is no more than *counting the numbers of ways the data could happen*, **according** to **our assumptions**.

**This allows** us to use **probability theory** as a *general way* to *represent plausibility*, whether in *reference to countable events* in the world **or** rather *theoretical constructs like parameters*.

More generally, **Bayesian golems treat “randomness”** as a *property of information*, *not of the world*.









#### In opposition to **Frequentist**:
**The frequentist approach** requires that **all probabilities** be *defined by connection to the frequencies* of **events** in very **large samples**.
This leads to **frequentist uncertainty** being *premised* on **imaginary resampling of data** - If we were to **repeat** the **measurement** many *many times*, we would *end up collecting* a **list of values** that *will have* some **pattern** to it.
- This is called a **list of values** is called a *SAMPLING DISTRIBUTION*.
